{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58308041",
      "metadata": {
        "id": "58308041"
      },
      "source": [
        "\n",
        "# Cholera Early‑Warning → SMS Alert (Demo Notebook)\n",
        "\n",
        "This notebook shows **how a predictive model can trigger SMS alerts** for cholera risk in Nigerian regions using:\n",
        "1) **Model creation** on historical-like (mocked) data\n",
        "2) **Loading + scoring** with near real-time weather forecasts (mocked)\n",
        "\n",
        "> Why this matters:\n",
        "- **Sparse diagnoses**: confirmed cholera diagnoses are sporadic and lag reality, so a *probabilistic early-warning* helps detect risk *before* official case confirmations.\n",
        "- **Supports contact tracing**: timely risk flags can route outreach and hygiene messaging to likely hotspots, accelerating case finding and **breaking transmission chains**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3327e00",
      "metadata": {
        "id": "f3327e00"
      },
      "source": [
        "\n",
        "> **Known limitations (to be transparent in the repo):**\n",
        "- **Reliant on user reporting / SMS access:** outreach assumes people own phones and opt-in; network coverage and literacy vary.\n",
        "- **Changing weather patterns:** climate regime shifts and anomalous seasons can drift relationships; models need **ongoing retraining**.\n",
        "- **Sparse local sub-regional data:** many covariates are coarse; sub‑LGA variability (informal settlements, borehole failures) is under-captured → **uncertainty** remains.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e484a78d",
      "metadata": {
        "id": "e484a78d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, RocCurveDisplay\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "# Plotting (follow org conventions: matplotlib only, single chart per cell, no explicit colors)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "rng = np.random.default_rng(42)\n",
        "DATA_DIR = Path('/mnt/data')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935a8077",
      "metadata": {
        "id": "935a8077"
      },
      "source": [
        "\n",
        "## 1) Mock Historical Data (Training Set)\n",
        "\n",
        "We create a toy weekly panel at the **state** level with these features (all per‑capita where noted):\n",
        "- `rainfall_mm` (weekly forecast/estimate; we also create lag terms)\n",
        "- `flood_risk` (0–1)\n",
        "- `water_access_pc` (per 1k people with clean water)\n",
        "- `health_facilities_pc` (per 1k people)\n",
        "- **Label:** `outbreak_next_2w` (1 if an outbreak emerges within the next two weeks)\n",
        "\n",
        "> In production, replace this block by joining official case line‑lists (NCDC), ERA5 rainfall/flood indices, WASH coverage, and facility counts, all normalized by population and aligned to **region × week**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a890a36d",
      "metadata": {
        "id": "a890a36d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create mock regions and weeks\n",
        "states = [f'State_{i:02d}' for i in range(1, 20)]  # 19 states for demo\n",
        "weeks = pd.date_range('2023-01-01', periods=60, freq='W')  # ~60 weeks\n",
        "\n",
        "rows = []\n",
        "for s in states:\n",
        "    base_water = rng.uniform(20, 80)  # per 1k\n",
        "    base_fac = rng.uniform(0.1, 0.6)  # per 1k\n",
        "    flood_profile = rng.uniform(0.2, 0.8)\n",
        "    for w in weeks:\n",
        "        rainfall = max(0, rng.normal(60, 30))  # mm/week\n",
        "        flood = np.clip(flood_profile + 0.003 * (rainfall - 60) + rng.normal(0, 0.05), 0, 1)\n",
        "        water_access = np.clip(base_water + rng.normal(0, 5), 0, 100)\n",
        "        health_fac_pc = np.clip(base_fac + rng.normal(0, 0.05), 0, 2)\n",
        "\n",
        "        # Latent risk: more rainfall & flood -> higher risk; better water & facilities -> lower risk\n",
        "        logit = -2.2 + 0.012*rainfall + 1.3*flood - 0.015*water_access - 0.8*health_fac_pc\n",
        "        p = 1/(1 + np.exp(-logit))\n",
        "        outbreak = rng.binomial(1, p)\n",
        "\n",
        "        rows.append(dict(\n",
        "            state=s, week=w, rainfall_mm=rainfall, flood_risk=flood,\n",
        "            water_access_pc=water_access, health_facilities_pc=health_fac_pc,\n",
        "            outbreak_next_2w=outbreak\n",
        "        ))\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(['state','week']).reset_index(drop=True)\n",
        "\n",
        "# Simple feature lags\n",
        "df['rainfall_mm_lag1'] = df.groupby('state')['rainfall_mm'].shift(1).fillna(df['rainfall_mm'].median())\n",
        "df['flood_risk_lag1'] = df.groupby('state')['flood_risk'].shift(1).fillna(df['flood_risk'].median())\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff31b80",
      "metadata": {
        "id": "8ff31b80"
      },
      "source": [
        "\n",
        "## 2) Train a Predictive Model\n",
        "\n",
        "We use a small **Gradient Boosting** classifier wrapped in a pipeline with standardization for stability, then **calibrate** the probabilities (isotonic) for thresholding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b9de95",
      "metadata": {
        "id": "f6b9de95"
      },
      "outputs": [],
      "source": [
        "\n",
        "features = ['rainfall_mm','flood_risk','water_access_pc','health_facilities_pc',\n",
        "            'rainfall_mm_lag1','flood_risk_lag1']\n",
        "target = 'outbreak_next_2w'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7, stratify=y)\n",
        "\n",
        "# Pipeline: scale -> model\n",
        "base_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gb', GradientBoostingClassifier(random_state=7))\n",
        "])\n",
        "\n",
        "# Calibrated probabilities\n",
        "cal_model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)\n",
        "cal_model.fit(X_train, y_train)\n",
        "\n",
        "pred_proba = cal_model.predict_proba(X_test)[:,1]\n",
        "roc = roc_auc_score(y_test, pred_proba)\n",
        "ap = average_precision_score(y_test, pred_proba)\n",
        "\n",
        "print(f'ROC AUC: {roc:.3f} | Average Precision: {ap:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d8faf3",
      "metadata": {
        "id": "83d8faf3"
      },
      "source": [
        "\n",
        "### Choose Action Thresholds\n",
        "\n",
        "We’ll adopt the hackathon defaults:\n",
        "- **High risk ≥ 0.70** → trigger **public SMS** + notify officials\n",
        "- **Medium 0.40–0.69** → notify facilities; no mass SMS\n",
        "- **Low < 0.40** → routine surveillance\n",
        "\n",
        "Below we visualize the ROC just to sanity‑check separability (for production, add cost‑sensitive analysis).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda509ee",
      "metadata": {
        "id": "dda509ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "RocCurveDisplay.from_predictions(y_test, pred_proba)\n",
        "plt.title(\"ROC (holdout)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e753c249",
      "metadata": {
        "id": "e753c249"
      },
      "source": [
        "\n",
        "### Persist the Model\n",
        "\n",
        "We save the calibrated model and the feature list for reuse in the scoring step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84161c36",
      "metadata": {
        "id": "84161c36"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL_PATH = DATA_DIR / 'cholera_model.joblib'\n",
        "FEATURES_PATH = DATA_DIR / 'cholera_features.json'\n",
        "\n",
        "joblib.dump(cal_model, MODEL_PATH)\n",
        "FEATURES_PATH.write_text(json.dumps(features, indent=2))\n",
        "\n",
        "print(f\"Saved model → {MODEL_PATH}\")\n",
        "print(f\"Saved features → {FEATURES_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb036d3e",
      "metadata": {
        "id": "bb036d3e"
      },
      "source": [
        "\n",
        "## 3) Real‑Time Scoring with Forecasts\n",
        "\n",
        "Assume a weekly job pulls **weather forecasts** and merges with static covariates (flood risk baseline, WASH access, facilities per‑capita). Here we mock a single scoring week for a subset of states.\n",
        "\n",
        "> In production, replace the CSV creation below with your pipeline that ingests ERA5/Forecast API + registry tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08c6ee9",
      "metadata": {
        "id": "c08c6ee9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a mock forecast CSV for the coming week\n",
        "scoring_states = states[:8]\n",
        "score_rows = []\n",
        "future_week = pd.to_datetime('2025-09-28')  # example next week\n",
        "\n",
        "for s in scoring_states:\n",
        "    # Pull last week's values from df to create lags\n",
        "    hist = df[df['state']==s].iloc[-1]\n",
        "    rainfall_forecast = max(0, rng.normal(70, 25))  # wetter week\n",
        "\n",
        "    flood_forecast = float(np.clip(hist['flood_risk'] + 0.004*(rainfall_forecast - hist['rainfall_mm']) + rng.normal(0,0.04), 0, 1))\n",
        "\n",
        "    score_rows.append(dict(\n",
        "        state=s,\n",
        "        week=future_week,\n",
        "        rainfall_mm=rainfall_forecast,\n",
        "        flood_risk=flood_forecast,\n",
        "        water_access_pc=float(np.clip(hist['water_access_pc'] + rng.normal(0,2), 0, 100)),\n",
        "        health_facilities_pc=float(hist['health_facilities_pc']),\n",
        "        rainfall_mm_lag1=float(hist['rainfall_mm']),\n",
        "        flood_risk_lag1=float(hist['flood_risk'])\n",
        "    ))\n",
        "\n",
        "score_df = pd.DataFrame(score_rows)\n",
        "forecast_csv = DATA_DIR / 'weekly_forecast_input.csv'\n",
        "score_df.to_csv(forecast_csv, index=False)\n",
        "forecast_csv, score_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7373fb34",
      "metadata": {
        "id": "7373fb34"
      },
      "source": [
        "\n",
        "### Load Model & Score\n",
        "\n",
        "We produce **risk probabilities** and map them into **action tiers** that the backend can consume.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883473e0",
      "metadata": {
        "id": "883473e0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load artifacts\n",
        "model = joblib.load(MODEL_PATH)\n",
        "feat_list = json.loads(FEATURES_PATH.read_text())\n",
        "\n",
        "incoming = pd.read_csv(forecast_csv, parse_dates=['week'])\n",
        "\n",
        "# Predict probabilities\n",
        "probs = model.predict_proba(incoming[feat_list])[:,1]\n",
        "incoming['risk_prob'] = probs\n",
        "\n",
        "def to_action(p):\n",
        "    if p >= 0.70:\n",
        "        return 'HIGH: trigger_public_sms & notify_officials'\n",
        "    if p >= 0.40:\n",
        "        return 'MEDIUM: notify_facilities & monitor'\n",
        "    return 'LOW: routine_surveillance'\n",
        "\n",
        "incoming['action'] = incoming['risk_prob'].apply(to_action)\n",
        "incoming[['state','week','risk_prob','action']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b223ec14",
      "metadata": {
        "id": "b223ec14"
      },
      "source": [
        "\n",
        "### Backend Hook (Pseudo‑Trigger)\n",
        "\n",
        "In production, the dataframe below would be iterated and passed to your **SMS/alert service**. We stub a `send_alert()` function to show the interface signature you can wire to Twilio/Vonage/etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b6a2ea",
      "metadata": {
        "id": "45b6a2ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "def send_alert(region: str, week: str, prob: float, action: str):\n",
        "    \"\"\"Placeholder for your real SMS/notification publisher.\n",
        "    In prod, enforce idempotency + rate limiting, and log delivery + response.\n",
        "    \"\"\"\n",
        "    print(f\"[ALERT] {region} | {week} | risk={prob:.2f} → {action}\")\n",
        "    # TODO: integrate with your messaging bus / SMS provider\n",
        "\n",
        "# Fire only HIGH and MEDIUM actions\n",
        "to_fire = incoming[incoming['action'].str.startswith(('HIGH','MEDIUM'))]\n",
        "for _, row in to_fire.iterrows():\n",
        "    send_alert(row['state'], row['week'].date().isoformat(), row['risk_prob'], row['action'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef84d8a",
      "metadata": {
        "id": "4ef84d8a"
      },
      "source": [
        "\n",
        "### Export Scored Output\n",
        "\n",
        "Downstream services can pick up a simple CSV with `state, week, risk_prob, action`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d775c7",
      "metadata": {
        "id": "10d775c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "scored_csv = DATA_DIR / 'weekly_scored_output.csv'\n",
        "incoming[['state','week','risk_prob','action']].to_csv(scored_csv, index=False)\n",
        "print(f\"Wrote: {scored_csv}\")\n",
        "incoming[['state','week','risk_prob','action']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551c5c3a",
      "metadata": {
        "id": "551c5c3a"
      },
      "source": [
        "\n",
        "## Next Steps (productionizing)\n",
        "- Replace mocked joins with your **ETL** that aligns *region × week* across sources; validate population normalizations.\n",
        "- Add **drift monitoring** and **scheduled retraining** (e.g., monthly) as new case data arrives.\n",
        "- Implement **cost-aware thresholds** co-designed with health authorities (false alarm vs missed detection trade-offs).\n",
        "- Log **alert outcomes** (deliveries, clinic presentations) to improve the model via closed-loop learning.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}